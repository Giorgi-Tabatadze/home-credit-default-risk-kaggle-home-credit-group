{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import featuretools as ft\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings('ignore', category=FutureWarning, module='woodwork.*')\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='woodwork.*')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade numpy pandas matplotlib seaborn woodwork featuretools scikit-learn pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade dask distributed nodejs dask-labextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entityset_from_partition(path):\n",
    "    \n",
    "    \"\"\"Create an EntitySet from a partition of data specified as a path.\n",
    "      Returns a dictionary with the entityset and the number used for saving the feature matrix.\"\"\"\n",
    "    warnings.filterwarnings('ignore')\n",
    "    warnings.filterwarnings('ignore', category=FutureWarning, module='woodwork.*')\n",
    "    warnings.filterwarnings('ignore', category=UserWarning, module='woodwork.*')\n",
    "    partition_num = int(path[20:])\n",
    "    \n",
    "    # Read in data\n",
    "    app = pd.read_csv('%s/app.csv' % path)\n",
    "    bureau = pd.read_csv('%s/bureau.csv' % path)\n",
    "    bureau_balance = pd.read_csv('%s/bureau_balance.csv' % path)\n",
    "    previous = pd.read_csv('%s/previous.csv' % path)\n",
    "    credit = pd.read_csv('%s/credit.csv' % path)\n",
    "    installments = pd.read_csv('%s/installments.csv' % path)\n",
    "    cash = pd.read_csv('%s/cash.csv' % path)\n",
    "    \n",
    "    \n",
    "    # Empty entityset\n",
    "    es = ft.EntitySet(id = 'clients')\n",
    "    \n",
    "    es = es.add_dataframe(dataframe_name = 'app', dataframe = app, \n",
    "                                 index = 'SK_ID_CURR',)\n",
    "    es = es.add_dataframe(dataframe_name = 'bureau', dataframe = bureau, \n",
    "                                 index = 'SK_ID_BUREAU', time_index='bureau_credit_application_date')\n",
    "    \n",
    "    es = es.add_dataframe(dataframe_name = 'previous', dataframe = previous, \n",
    "                                 index = 'SK_ID_PREV', time_index = 'previous_decision_date',)\n",
    "    \n",
    "    # Entities that do not have a unique index\n",
    "    es = es.add_dataframe(dataframe_name = 'bureau_balance', dataframe = bureau_balance, \n",
    "                                 make_index = True, index = 'bb_index', time_index = 'bureau_balance_date'\n",
    "                                 )\n",
    "    \n",
    "    es = es.add_dataframe(dataframe_name = 'cash', dataframe = cash, \n",
    "                                 make_index = True, index = 'cash_index', time_index = 'cash_balance_date'\n",
    "                                 )\n",
    "    \n",
    "    es = es.add_dataframe(dataframe_name = 'installments', dataframe = installments,\n",
    "                                 make_index = True, index = 'installments_index', time_index = 'installments_paid_date'\n",
    "                                 )\n",
    "    \n",
    "    es = es.add_dataframe(dataframe_name = 'credit', dataframe = credit,\n",
    "                                 make_index = True, index = 'credit_index', time_index = 'credit_balance_date'\n",
    "                                 )\n",
    "    \n",
    "    es = es.add_relationship(parent_dataframe_name='app', \n",
    "                                 parent_column_name='SK_ID_CURR', \n",
    "                                 child_dataframe_name='bureau', \n",
    "                                 child_column_name='SK_ID_CURR')\n",
    "    \n",
    "    es = es.add_relationship(parent_dataframe_name='bureau', \n",
    "                                    parent_column_name='SK_ID_BUREAU', \n",
    "                                    child_dataframe_name='bureau_balance', \n",
    "                                    child_column_name='SK_ID_BUREAU')\n",
    "    es = es.add_relationship(parent_dataframe_name='app', \n",
    "                                    parent_column_name='SK_ID_CURR', \n",
    "                                    child_dataframe_name='previous', \n",
    "                                    child_column_name='SK_ID_CURR')\n",
    "    es = es.add_relationship(parent_dataframe_name='previous', \n",
    "                                    parent_column_name='SK_ID_PREV', \n",
    "                                    child_dataframe_name='cash', \n",
    "                                    child_column_name='SK_ID_PREV')\n",
    "    es = es.add_relationship(parent_dataframe_name='previous', \n",
    "                                             parent_column_name='SK_ID_PREV', \n",
    "                                             child_dataframe_name='installments', \n",
    "                                             child_column_name='SK_ID_PREV')\n",
    "    es = es.add_relationship(parent_dataframe_name='previous', \n",
    "                                       parent_column_name='SK_ID_PREV', \n",
    "                                       child_dataframe_name='credit', \n",
    "                                       child_column_name='SK_ID_PREV')\n",
    "\n",
    "    return ({'es': es, 'num': partition_num})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_matrix_from_entityset(es_dict, feature_defs, return_fm = False):\n",
    "    import os\n",
    "\n",
    "    \"\"\"Run deep feature synthesis from an entityset and feature definitions. \n",
    "    Saves feature matrix based on partition.\"\"\" \n",
    "    \n",
    "    # Extract the entityset\n",
    "    es = es_dict['es']\n",
    "    \n",
    "    #Calculate the feature matrix and save\n",
    "    feature_matrix = ft.calculate_feature_matrix(feature_defs,\n",
    "                                                 entityset=es, \n",
    "                                                 n_jobs = 1, \n",
    "                                                 verbose = 0,\n",
    "                                                 chunk_size = es['app'].shape[0])\n",
    "    \n",
    "    directory = './input/tm'\n",
    "    if os.path.exists(directory):\n",
    "        print(\"directory exists\")\n",
    "    else:\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    feature_matrix.to_csv('./input/tm/t%d_fm.csv' % es_dict['num'], index = True)\n",
    "    \n",
    "    if return_fm:\n",
    "        return feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450\n"
     ]
    }
   ],
   "source": [
    "feature_defs = ft.load_features('./input/time_features.txt')\n",
    "print(len(feature_defs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.bag as db\n",
    "from dask.distributed import Client\n",
    "\n",
    "# Use all 8 cores\n",
    "client = Client(processes = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = ['./input/partitions/t%d' %  i for i in range(90, 105)]\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bag object\n",
    "b = db.from_sequence(paths)\n",
    "\n",
    "# Map entityset function\n",
    "b = b.map(entityset_from_partition)\n",
    "\n",
    "# Map feature matrix function\n",
    "b = b.map(feature_matrix_from_entityset, feature_defs = feature_defs)\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore', FutureWarning) \n",
    "    b.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = './input/tm/'\n",
    "fm_paths = [base + p for p in os.listdir(base) if 'tm.csv' in p]\n",
    "fms = [pd.read_csv(path) for path in fm_paths]\n",
    "feature_matrix = pd.concat(fms, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix.to_csv(\"./input/time_features.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
